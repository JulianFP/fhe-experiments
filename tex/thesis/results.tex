\chapter{Results}\label{chap:results}

We executed the described experiment suite on two different machines to get a more complete picture of \ac{fhe} runtime behavior:
\begin{itemize}
    \item Our working groups cluster with 2x Intel Xeon Gold 6230R CPUs (in total 52 cores/104 threads @2.1GHz, up to 4.0GHz) with 192GB of memory, representing a machine with very high multi-core but lower single-core performance
    \item A personal computer with 1x AMD Ryzen 5 7600 CPU (6 cores/12 threads @3.8GHz, up to 5.1GHz) with 32GB of memory, representing a machine with much lower multi-core but higher single-core performance
\end{itemize}

A lot of \ac{fhe} operations in concrete are very well parallelized while our clear executions of the same models are not.
Consequently we wanted to test the runtime on two configurations, one being more fair for the comparison between \ac{fhe} and clear executions since the \ac{fhe} execution cannot gain that much by parallelization on a small core-count CPU, and one representing a more realistic deployment for \ac{fhe} applications since server and cloud machines tend to have high core counts but smaller single core performance.
We also want to point out that the cluster was being used by other people as well during the experiments, introducing a higher runtime variance between the executions.
The personal computer on the other hand was a more controlled environment with no other users and as little processes as possible running at the same time.
Due to this we expect the standard deviation (plotted as error bars in our plots) to be much lower on the PC runs.

We executed every experiment 10 times and calculated the average of all values as well as their standard deviation.
Note that the error bars in all plots represent this standard deviation.

\section{Encrypted Inference with Built-In Models}

These are the results of our first group of experiments, described in \cref{encrypted_inference}.
During reading the results please note that we did not execute the Neural Network experiment (see \cref{neural_net_exp}) on all variations of the SMS Spam dataset (only up to a feature size of 100) because of time constraints.

\subsection{Accuracy and F1-Score}

In \cref{fig:cluster_acc_f1} we can see an overview of the accuracy and F1-Scores of our three models from the cluster run.
A full overview of all the data is available in \cref{table:inference_cluster_acc-f1} (cluster run) and \cref{table:inference_PC_acc-f1} (PC run).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_Logistic Regression.pdf}
        \end{center}
        \caption{Logistic Regression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{XGB Tree-based}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_Neural Network Classifier.pdf}
        \end{center}
        \caption{Neural Network}
    \end{subfigure}
    \caption{Accuracy and F1-Scores of all of our three models in both clear and FHE on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_acc_f1}
\end{figure}

As expected (see \cref{LogisticRegressionExp}) the Logistic Regression model quality did not suffer from compiling it to its \ac{fhe} equivalent.
For all datasets, the accuracy and F1-scores stay exactly the same while also having a no standard deviation (since we fixed the seed for this experiment).
This proofs that with a high quantization bit-width and a very shallow \ac{fhe} inference circuit one can achieve perfect accuracy of the \ac{fhe} model.

Our \ac{cart} model (see \cref{XGBExp}) only suffers from an extremely small accuracy loss due to quantization which is not viewable on the plot.
While this was better than expected, we also have to note that the \ac{pca} that we used in this experiment might have helped a lot here.
In fact it might have even been much too aggressive since in theory any \ac{cart} model must be at least equally as good and most of the time better than Logistic Regression.
What we observed instead were changing improvements over Logistic Regression on some datasets (xor, most spam variants), but a worse performance than Logistic Regression on the others.
As with most experiments we implemented them as close as the example code from the concrete-ml documentation.
In this case however the recommended usage of \ac{pca} probably crippled the model quite a bit to the point were using it over Logistic Regression makes no sense considering its increased runtimes.

For our \ac{fnn} model (see \cref{neural_net_exp}) experiments we did not use a fixed seed since it is not based on scikit-learn anymore and thus concrete-ml does not expose an attribute for doing this.
As a result our accuracies and F1-scores were subject to variation from run to run which can be seen in the error bars.
The \ac{fnn} model has slightly better accuracies across the board, except for the spam and cancer datasets where it is slightly behind Logistic Regression.
The differences are so small however that they are mostly insignificant hinting at the fact that our datasets are not challenging enough to show the differences in model performance well.
Notably the neural network is the only model having acceptable accuracy in our XOR dataset.
As expected the accuracy drops quite much after compiling the model to \ac{fhe}.
How much was highly dependent on the dataset.
While the digits and spam datasets where barely affected, the \ac{fhe} model performed much poorer than the clear model on the cancer and xor datasets which are very challenging for the quantization.

Except for smaller variations within the margin of error, the PC run returned the same accuracy and F1-scores.

\subsection{Runtime}

A full overview of all runtime data is available in \cref{table:inference_cluster_dur} (cluster run) and \cref{table:inference_PC_dur} (PC run).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_Logistic Regression.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_Logistic Regression.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the Logistic Regression model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_logReg}
\end{figure}

The runtime increases due to \ac{fhe} of our Logistic Regression experiment (see \cref{fig:cluster_runtime_logReg}) started out to be relatively modest on datasets with smaller feature sizes with slowdowns around $100\times$.
However even this kind of slowdown means in practice that an extremely trivial operation like a vector product and addition (i.e. inference of a Logistic Regression model) that (for the whole test dataset) usually takes single- to low two-digit milliseconds becomes an operation taking a good fraction of a second, or even more than a second.
This also does not include the pre-processing (e.g. encryption) cost, that is higher than the clear processing by similar factors.
Interestingly the relative slowdown increases drastically with larger feature sizes, suggesting that computing a vector product in \ac{fhe} becomes increasingly inefficient with larger dimensions.
The PC run (see \cref{fig:PC_runtime_logReg}) was faster for the clear execution than on the cluster, while the \ac{fhe} execution was slower, both by a significant margin.
This increases the slowdown factors even more, suggesting that while the clear model benefits from the stronger single-core performance of the PC, \ac{fhe} is much better parallelized and thus can benefit much more of the high core-count of the cluster.
This is even more apparent for the \ac{fhe} pre-processing.
While post-processing benefits from higher single-core performance and experiences a significant performance improvement on the PC, pre-processing slows down so much more that in total our relative pre+post numbers increase by quite a lot compared to the cluster run.
While most cloud machines offer high multi-core performance that \ac{fhe} processing can take advantage from, the pre-processing runs on client devices where parallelization results in much less of a performance boost.
This means that specifically for the pre+post processing results, the PC run is much more representative for real-world conditions than the cluster run.
Not only does this mean that the pre+post processing is much slower than just running the clear model directly on the client, for Logistic Regression it is also even slower than the actual \ac{fhe} processing, raising the question whether such simple models are even worth running in a homomorphic environment.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the XGB tree-based model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_xgb}
\end{figure}

The \ac{cart} models shows a bit of a different runtime behavior (see \cref{fig:cluster_runtime_xgb}).
First of all, the runtime does not increase with dataset feature size because the effective feature dimensionality to (almost) the same value by applying \ac{pca}.
However the Digits dataset still stands out with higher relative runtimes than the rest, suggesting that \ac{fhe} runtimes react much worse to larger class counts than the clear execution does.
The Synthetic dataset on the other hand is the one with the smallest relative slowdown which might have to do with it being easily separable with even very shallow decision trees (even for larger feature sizes).
The slowdown due to \ac{fhe} is also much larger for this model, being a full order of magnitude larger than for Logistic Regression (around $1000\times$ slower now).
This might be due to the necessary parallel execution of all possible branches of the tree in the \ac{fhe} model, or just due to the fact that the amount of computations required is higher than in Logistic Regression (even in a single branch).
At the same time the preprocessing duration has only increased slightly compared to Logistic Regression making the runtime proportions between clear, preprocessing and actual processing much more sensible in this experiment.
We still want to point out though that preprocessing was still by multiple factors slower than the clear execution of the model.
The PC run (see \cref{fig:PC_runtime_xgb}) does not differ that much this time, it essentially just has increased runtimes for everything across the board, including the clear execution this time.
Both the clear and \ac{fhe} executions seem to benefit more from higher core counts, changing the runtime ratios not that much in the process.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_Neural Network Classifier.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_Neural Network Classifier.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the Neural Network model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_neural_net}
\end{figure}

For our \ac{fnn} model, we make another jump by an order of magnitude regarding relative slowdown, now being in the $\approx 10,000\times$ range of slowdown and even hitting more than $100,000\times$ on the 100 dimensions variation of Spam (see \cref{fig:cluster_runtime_neural_net}).
This is also the reason why we quit our executions here, the slowdown became higher and higher with more dimensions, exploding this experiments execution times.
However the variance is also much larger here, both between the $10$ executions and between the datasets.
For example the relative slowdown is much lower for the smaller and simpler datasets like Iris, Cancer or our Synthetic dataset.
The Digits dataset, the one with the highest amount of classes, stands out here again because of especially large slowdowns, as well as the SMS Spam dataset variations.
Our PC run was slower across the board, however the \ac{fhe} execution suffered more from the lower core-count than the clear execution making the slowdown factors even worse while also featuring less variance/error across the $10$ executions (see \cref{fig:PC_runtime_neural_net}).

\subsection{How Runtime Grows with Feature Size}

Since we executed our Synthetic and Spam datasets with a variety of feature sizes, we can look at how runtimes grow with the dimensionality of the input features (see \cref{fig:cluster_synth_feature-runtime}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_Logistic Regression_synth_.pdf}
        \end{center}
        \caption{Logistic Regression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_XGB Tree-based Classification_synth_.pdf}
        \end{center}
        \caption{XGB Tree-based}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_Neural Network Classifier_synth_.pdf}
        \end{center}
        \caption{Neural Network}
    \end{subfigure}
    \caption{Runtimes plotted against feature size of the 'Synthetic' dataset of all of our three models in both clear and FHE, plotted on a logarithmic scale, drawn from the cluster run of the experiments}\label{fig:cluster_synth_feature-runtime}
\end{figure}

While clear execution and post-processing are not affected that much by the feature size, \ac{fhe} preprocessing and processing increase linearly and steep with growing feature size.
While of course in absolute values these increases are much higher for the \ac{fnn} model than for the Logistic Regression model, plotted on their respective scales the graphs look quite similar since the relative increases are quite similar.

Our \ac{cart} model is an exception here because of the applied \ac{pca} which normalizes feature size, as the model sees it, to a more or less fixed value.
We suspect that the chaotic changes in execution times are due to the for some dimensionalities the dataset is easier to separate with shallower trees(after applying \ac{pca}) than for others.

These findings also translate to the SMS dataset, as well as the plots for both datasets of the PC run (see \cref{fig:cluster_spam_feature-runtime}, \cref{fig:PC_synth_feature-runtime}, and \cref{fig:PC_spam_feature-runtime} respectively).

We also have to point out though that feature sizes of larger than $250$ are unusual in practice and where integrated into the benchmarks more to push the boundaries of what is possible with this framework, and less to reflect real-world relevant use cases.
We also observe that increasing the feature size over $250$ on the Spam dataset did not increase accuracy anymore, with $100$ and $50$ dimensions only resulting in relatively minor accuracy losses but quite high runtime gains during \ac{fhe} execution with all model types.

\subsection{Changes in the Decision Boundary}

We will now look at the decision boundaries of the trained models to find some visual explanations for the observed accuracy losses of the models.
For this we used \ac{pca} backwards to map 2-dimensional coordinates to features for the dataset (except for XOR where the features already are 2-dimensional).
Unfortunately this did not succeed for the \ac{fnn} model though due to concrete-ml consistently throwing errors during plotting.
We suspect that the underlying cause might here might be the aggressive quantization used for the neural network experiment which restricts most of our \ac{pca}-mapped features to be in the expected input range for proper quantization.
Nevertheless even for Logistic Regression and our \ac{cart} models we see some interesting effects that quantization and \ac{fhe} compilation has on the decision boundary:

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Iris - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Iris - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Iris' dataset}\label{fig:dec_bound_log-reg_iris}
\end{figure}

As you can see in \cref{fig:dec_bound_log-reg_iris}, for Logistic Regression the changes in the decision boundary mostly manifest in the boundary being a bit more granular due to quantization.
There are however exceptions to this rule, as one can see on the breast cancer dataset which has quite large feature dimensions and thus also sees very high changes in boundary granularity (see \cref{fig:dec_bound_log-reg_cancer}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Breast Cancer - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Breast Cancer - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Breast Cancer' dataset}\label{fig:dec_bound_log-reg_cancer}
\end{figure}

Another exception is the digits model, where due to quantization the boundary changes quite drastically at the edges of the feature space (see \cref{fig:dec_bound_log-reg_digits}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Digits - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Digits - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Digits' dataset}\label{fig:dec_bound_log-reg_digits}
\end{figure}

For the \ac{cart} model, the decision boundaries change much less after compiling the model to its \ac{fhe} equivalent.
This may be attributes to the fact that the boundaries are already quite sharp due to the model design.
Every decision in our decision tree creates a sharp, straight line whether the threshold has been quantized or not.
However quantization can still move these straight lines around a bit, as can be seen on the Digits dataset (see \cref{fig:dec_bound_xgb_digits})

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/XGB Tree-based Classification - Digits - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/XGB Tree-based Classification - Digits - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} XGB tree-based model on the 'Digits' dataset}\label{fig:dec_bound_xgb_digits}
\end{figure}

More decision boundaries of some of our datasets for these two models can be found in \cref{add_dec_bound}.

\section{Encrypted Training with Built-In Model}

These are the results of our second group of experiments, described in \cref{encrypted_training}.
Please note that we were not able to run encrypted training on the 'Breast Cancer' dataset or on the 'Synthetic' dataset for features with more than $250$ dimensions (see \cref{encrypted_training_limitations}).
Like with encrypted inference on the \ac{fnn} model we also did not run this experiment on the 'SMS Spam' dataset for feature dimensions larger than $100$, again because of time restrictions.
For the same reason we also only executed this experiment on the cluster and not on the PC.

\subsection{Limitations of concrete-ml and Encountered Errors}\label{encrypted_training_limitations}

Encrypted \ac{sgd} training failed for some of our datasets due to two separate errors:

\begin{lstlisting}[caption={The thrown Exception while training on the encrypted 'Synthetic, 500 features' dataset}, label=code:training_synth_error, escapeinside={[*}{*]}]
RuntimeError: Unfeasible noise constraint encountered (see https://docs.zama.ai/concrete/compilation/common_errors#id-8.-unfeasible-noise-constraint): At location quantizers.py:717:0:
[*$1\sigma^2Br[1] + 1\sigma^2Br[2] + 1\sigma^2K[1\rightarrow4] + 1\sigma^2FK[2\rightarrow1] + 1\sigma^2M[4] < (2^2)**-15.5$*] (12bits partition:4 count:500, dom=31).
\end{lstlisting}

For larger feature dimensions (in our experiment suite beginning at $500$) concrete-ml failed during model training (\cref{code:training_synth_error}).
This was due to an unfeasible noise constraint which is a documented error in concrete complaining about a defined \ac{fhe} circuit that concrete cannot find cryptographic parameters for that are both secure and maintain correctness during decryption \cite{noauthor_concrete_2025-1}.
The suggested fixes all require modifying the \ac{fhe} circuit which is why we suspect that concrete-ml compiled our training into an incorrect \ac{fhe} circuit suggesting an issue/bug in concrete-ml rather than in our usage of it.

\begin{lstlisting}[caption={The thrown Exception while compiling the training circuit for the 'Breast Cancer' dataset}, label=code:training_cancer_error]
RuntimeError: Function you are trying to compile cannot be compiled
\end{lstlisting}

For the 'Breast Cancer' dataset concrete-ml already failed to compile the training circuit with an unhelpful exception message (\cref{code:training_cancer_error}).
This is probably due to the high spread of the features in the dataset, and we were also not able to remedy this by reducing the quantization bit-width.

We can conclude that the encrypted training of concrete-ml cannot be executed on arbitrary datasets and for arbitrary tasks since we were only able to train on datasets that have certain feature properties.

\subsection{Accuracy and F1-Score}

In \cref{fig:cluster_acc_f1_training} we can see an overview of the accuracy and F1-Scores of the model that resulted from encrypted \ac{sgd} training compared to a model that was trained with \ac{sgd} on clear data.
A full overview of all the data can be found in \cref{table:training_cluster_acc-f1}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/cluster_acc_f1-plot_SGD Classifier Training.pdf}
    \caption{Accuracy and F1-Scores of all of encrypted \ac{sgd} training in both clear and FHE on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_acc_f1_training}
\end{figure}

Our experiment showed that \ac{fhe}-training had no impact on model accuracy during later inference, both models performed exactly the same on all our datasets.
That being said, if we compare these values with our Logistic Regression model in \cref{fig:cluster_acc_f1} we can see a clear accuracy reduction.
On our XOR dataset, these models even achieved an F1-Score of $0$.
The reason for this is that while during inference these models are effectively the same, during training \ac{sgd} is only an approximate algorithm while the earlier Logistic Regression model is being fitted exactly on the training data.
\ac{sgd} is however probably used here because of it allows online learning, thus our \ac{fhe} circuit only needs to consider the current model weights and the current sample for weight updates, not all training samples at once.
However for larger datasets \ac{sgd} becomes a requirement anyway, making this an acceptable compromise for many use-cases.

\subsection{Runtime}

\Cref{fig:cluster_runtime_training} shows an overview of the runtimes of encrypted \ac{sgd} training.
The full data is available in \cref{table:training_cluster_dur}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_SGD Classifier Training.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_SGD Classifier Training.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the encrypted \ac{sgd} training on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_training}
\end{figure}

As we can see the runtime increases are much greater for encrypted training than for the encrypted inference looked at before.
How much it is exactly varied much and dependent on both the size of the training dataset and the dimensionality of the features.
For anything more complex than XOR and Iris \ac{fhe} processing was around $500,000\times$ slower than clear execution, with the slowdown peaking at $1,585,672\times$ for the 'Synthetic' dataset with $250$ features.
This is the largest slowdown that we have seen throughout this thesis, especially considering that this was executed on the cluster and not the PC.
The plot shows a much exaggerated error for this data which is a result from the high standard deviation on the clear execution, not the \ac{fhe} execution.
This is probably an outlier due to other users putting or removing from the server during this time in the experiment run.
Since the clear execution time seems quite sensible compared to the other datasets its actual impact on the data was probably not nearly as large as shown in the plot.

Again we observe that, even for a more computationally demanding task like model training, running the task locally would have resulted in less computational burden on the client because the pre-processing of the training samples and post-processing of the resulting weights were already slower than the model training itself.

\subsection{How Runtime Grows with Feature Size}

Because of the limitations discussed in \cref{encrypted_training_limitations} and runtime concerns we were not able to run encrypted training on too many variations of our 'Synthetic' and 'SMS Spam' datasets.
From what we have seen though encrypted training runtime seems to grow linearly with the feature dimensionality like it did for the Logistic Regression and \ac{fnn} models in encrypted inference.
The plots for this can be found in \cref{fig:cluster_training_synth_spam_feature-runtime}.

\section{Encrypted Inference with custom approaches for the \acl{ner} task}

These are the results of our third and final group of experiments, described in \cref{ner_exps}.
Please note that we were only able to run our custom PyTorch model using \ac{ptq}, and our \ac{knn} approach with very unrepresentative and practically useless parameters (see \cref{ner_limitations}).

\subsection{Limitations of concrete-ml and Encountered Errors}\label{ner_limitations}

\subsubsection{Imposed Restrictions to PyTorch Model Design}

We already described our custom model in \cref{custom_pytorch_model}, however originally we built the architecture a bit differently and also achieved slightly better results with it.
We had to simplify the model design however because of compatibility issues with concrete-ml.

Essentially we tried to supply the \ac{fnn}-part of the model with our three types of features separately while using separate embeddings on the token indices and capitalization information and passing the word lengths directly to the model without an embedding.
This original model design can be seen in \cref{code:original_model}.
In theory this should be a cleaner solution since between these three categories the inputs have drastically different meanings and their relation to one another (across these categories) does not matter at all.

\begin{lstlisting}[caption={The thrown Exception while compiling the inference circuit for our original \ac{ner} PyTorch model}, label=code:ner_original_model_error]
AssertionError: Values must be float if value_is_float is set to True, got int64: <the torch inputset>
\end{lstlisting}

While our original approach worked well in clear (even better than our final approach), concrete-ml was not able to compile the model to its \ac{fhe} equivalent (\cref{code:ner_original_model_error}).
The exception complains about the inputs not being floating point numbers even though integers are a requirement for PyTorch's Embedding layer (which is why casting the features into floats also would not work here).
Concrete-ml does support the PyTorch Embedding layer, but only if it is the very first operation of the forward method.
Since our approach required splitting the features into three separate vectors before passing them to the Embedding layer, it did not compile with an obscure error message.
At the time of writing this restriction is not documented anywhere and we were only able to confirm that this is intended behavior and not a bug of the framework by conferring with a developer of concrete-ml on the FHE.org Discord server.

We worked around this by modifying our features such that there would be no collision between vocabulary indices, capitalization integers, and word lengths (by just using integers larger then the size of the vocabulary for the latter two) and passing all features directly through one shared Embedding layer.

\subsubsection{\ac{qat} not compatible with PyTorch Embeddings}

Initially we wanted to compare \ac{ptq} with \ac{qat} performance on our custom PyTorch model.
However after implementing and training the \ac{qat} version of our model, compiling it into its \ac{fhe} equivalent failed due to the \ac{qat} code not being compatible with the PyTorch Embedding and its integer inputs.
Since most other models take floating point inputs that require different quantization, the PyTorch Embedding requires some special treatment by concrete-ml.
At the time of writing this has not been adapted to the \ac{qat} code yet.
Therefore we were not able to include the \ac{qat} version of our model in the results, however it should be noted that we already tested the \ac{qat} capabilities of concrete-ml by using its Built-In \ac{fnn} model in \cref{encrypted_inference}.

\subsubsection{A Discovered Bug}

A second issue related to PyTorch Embeddings turned up after compiling the model into its \ac{fhe} equivalent while using the client-server setup in the experiment.
Using the custom model in a deployment scenario failed while using the testing method of the model for inference worked.
After consulting the FHE.org Discord server this turned out to be a bug, and was fixed in \href{https://github.com/zama-ai/concrete-ml/pull/1149}{this GitHub Pull Request}.

\subsubsection{Concrete-ml's \ac{knn} Turning Out to be Useless in Practice}\label{knn_limitation}

Regarding our second \ac{ner} approach described in \cref{ner_knn-approach} we have to report that we were not able to implement it for parameters that are meaningful for real-world scenarios like our \ac{ner} problem in any way.
Concrete-ml was only able to compile and train our model ones we reduced the training dataset to $13$ samples of $3$ dimensions each (using \ac{pca}).
For reference, \ac{bert} embeddings have 768 dimensions and our \ac{cconll} training set has almost $60,000$ samples.
While reducing these values somewhat, e.g. due to the curse of dimensionality described in \cref{KNN}, might not harm model performance much, these values are of course completely useless for our purposes (or any other real-world purposes for a \ac{knn} model).

While apparently the \ac{knn} model is still quite new and still being worked on in concrete-ml, these limitations are not mentioned in the documentation and we only found out about them after already implementing the experiment.
Because of this and to still find out about the runtime performance of this \ac{fhe} \ac{knn} model we chose to still include this experiment in this work since it might still be representative of feature versions of concrete-ml, or other \ac{fhe} implementations of \ac{knn}.

\subsection{Accuracy and F1-Score}

\Cref{fig:cluster_acc_f1_ner} shows an overview of the accuracy and F1-Scores of two \ac{ner} approaches from the cluster run. The full data is available in \cref{table:ner_cluster_acc-f1} (cluster run) and \cref{table:ner_PC_acc-f1} (PC run).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/cluster_acc_f1-plot-ner.pdf}
    \caption{Accuracy and F1-Scores of our two \ac{ner} approaches in both clear and FHE on the \ac{cconll} dataset, drawn from the cluster run of the experiments}\label{fig:cluster_acc_f1_ner}
\end{figure}

While we expected quite a large decrease of accuracy due to the use of \ac{ptq}, the large drop from an F1-Score of $80.8\%$ to $46.6\%$ still came at a surprise.
As a reference point, the \ac{knn} approach, which cannot really be better than random class selection with these parameters (see \cref{knn_limitation}), had a score of $35.9\%$; only about $10\%$ worse then our custom PyTorch model in \ac{fhe}.
This is quite the large gap in model accuracy between clear and \ac{fhe} and would have a considerable impact on the user experience if the \ac{fhe} model were deployed in a production environment.
Since we actually used a larger bit-width than for the \ac{fnn} model experiment we largely contribute this to the usage of \ac{ptq} over \ac{qat} and maybe to \ac{ner} as a use case which might be more sensitive to smaller weight and input changes due to quantization.
After all even smaller changes in the vocabulary index or in the embedding vector could mean a significant semantic distortion for the model.
Our PC run also delivered similar results (see \cref{fig:PC_acc_f1}).

While our \ac{knn} approach does not solve the \ac{ner} task at all (see \cref{knn_limitation}), it is worth to point out that the accuracy and F1-Score increased a bit in the homomorphic version of the model.
From this does not follow that \ac{fhe} improved the model in some way but rather that due to quantization the geometric arrangement of the training points got slightly altered, notching the model towards a better accuracy by pure chance.
However we suspect that for a much more complex \ac{knn} model that performs reasonably well on a given task quantization will less likely have a positive impact on model performance.
For example if the vectors are embeddings (semantic word embeddings or also visual embeddings) quantization would then slightly change the meaning of the training data points and the inference sample in an unpredictable way which will most likely not benefit the model.

\subsection{Runtime}

\Cref{fig:cluster_runtime_ner} shows an overview of the runtimes of our two \ac{ner} approaches from the cluster run. The full data is available in \cref{table:ner_cluster_dur} (cluster run) and \cref{table:ner_PC_dur} (PC run).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-ner.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-ner-with-ratio.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of our two \ac{ner} approaches in both clear and FHE on the \ac{cconll} dataset, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_ner}
\end{figure}

The relative slowdown for the processing step of our custom PyTorch model was in the same order of magnitude as that of the Built-In \ac{fnn} model in \cref{fig:cluster_runtime_neural_net}.
The pre- and post-processing however took much longer, with both the absolute runtime and relative slowdown being of similar magnitude than the \ac{fhe} processing itself.
This again raises the question of usability of this approach, because not only would it have meant almost $20,000\times$ less computation on the client-side if the client had just run the clear model itself, it also means that with the \ac{fhe} approach the client has to compute almost as long as the server.
On the PC run, these runtime values exploded even by multiple orders of magnitude to a slowdown of over $1,000,000\times$ for both the processing and preprocessing compared to clear model execution.
This significant difference between cluster and PC execution shows how much \ac{fhe} relies on performance improvements coming from parallelization for these larger and complexer models.
Because of these long runtimes, especially during the preprocessing step, we had set the testing set to only include $100$ random samples.
While this is not ideal for our accuracy measurements it was very much necessary to even being able to run these experiments since even for only $100$ samples these kind of slowdowns already meant multiple days of runtime for all $10$ executions of this experiment.
If we divide the preprocessing time from the PC run by $100$ we can easily see that if this model were deployed in production the user would have to wait almost $2$ minutes for their input to be encrypted and sent to the server, and probably much longer if they are on a less powerful machine like a laptop or smartphone.
If applied on a whole text snippet like an email this would have to be done for every word that the user wants the entity for (most of the time for every token in the text snippet).
While the \ac{fhe} processing happens on the server where the user does not necessarily have to wait for its completion, this client-side preprocessing happens client-side were the user would have to for example keep the browser tab of our imaginary \ac{ner} service open the whole time.
During this time the user machines CPU would also be burdened quite a lot which is especially problematic on mobile battery-powered devices.
The inefficiency of the \ac{fhe} preprocessing step is especially puzzeling if we remember that each sample only consists of $15$ integers ($3$ for each of the $5$ window tokens).
We do not know what causes quantization and encryption to take so long for such small input sizes, in particular because with the Built-In models the preprocessing runtime slowdowns were in completely different ranges.

For the \ac{knn} model the processing slowdowns were a bit less significant both in relative and specifically in absolute numbers.
The \ac{fhe} version of the \ac{knn} model actually performed better on the PC run which was surprising given that \ac{knn} usually involves a lot of branching which is implemented with parallelization in \ac{fhe}.
Still, considering the extremely low amount of samples in dimensions that we used the model with a runtime of almost half an hour for the $100$ samples is still far away of what for example Apple would probably consider as acceptable for their \ac{fhe} \ac{knn} application \cite{noauthor_combining_nodate}.
The solution used by Apple for \ac{fhe} was built using a very different \ac{fhe} scheme and a leveled approach.
They also used clustering datasets to reduce the amount of samples and dimensions of each of their clusters to mitigate runtimes and the curse of dimensionality that \ac{knn} suffers from in general.
Nevertheless it is still safe to conclude that concrete-ml's implementation of \ac{fhe} for \ac{knn} is far inferior to Apple's, even if we do not consider the low amount of samples and dimensions it supports.
