\chapter{Results}\label{chap:results}

We executed the described experiment suite on two different machines to get a more complete picture of \ac{fhe} runtime behavior:
\begin{itemize}
    \item Our working groups cluster with 2x Intel Xeon Gold 6230R CPUs (in total 52 cores/104 threads @2.1GHz, up to 4.0GHz) with 192GB of memory, representing a machine with very high multi-core but lower single-core performance
    \item A personal computer with 1x AMD Ryzen 5 7600 CPU (6 cores/12 threads @3.8GHz, up to 5.1GHz) with 32GB of memory, representing a machine with much lower multi-core but higher single-core performance
\end{itemize}

A lot of \ac{fhe} operations in concrete are very well parallelized while our clear executions of the same models are not.
Consequently we wanted to test the runtime on two configurations, one being more fair for the comparison between \ac{fhe} and clear executions since the \ac{fhe} execution cannot gain that much by parallelization on a small core-count CPU, and one representing a more realistic deployment for \ac{fhe} applications since server and cloud machines tend to have high core counts but smaller single core performance.
We also want to point out that the cluster was being used by other people as well during the experiments, introducing a higher runtime variance between the executions.
The personal computer on the other hand was a more controlled environment with no other users and as little processes as possible running at the same time.
Due to this we expect the standard deviation (plotted as error bars in our plots) to be much lower on the PC runs.

We executed every experiment 10 times and calculated the average of all values as well as their standard deviation.
Note that the error bars in all plots represent this standard deviation.

\section{Encrypted Inference with Built-In Models}

These are the results of our first group of experiments, described in \cref{encrypted_inference}.
During reading the results please note that we did not execute the Neural Network experiment (see \cref{neural_net_exp}) on all variations of the SMS Spam dataset (only up to a feature size of 100) because of time constraints.

\subsection{Accuracy and F1-Score}

In \cref{fig:cluster_acc_f1} we can see an overview of the accuracy and F1-Scores of our three models from the cluster run.
A full overview of all the data is available in \cref{table:inference_cluster_acc-f1} (cluster run) and \cref{table:inference_PC_acc-f1} (PC run).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_Logistic Regression.pdf}
        \end{center}
        \caption{Logistic Regression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{XGB Tree-based}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_acc_f1-plot_Neural Network Classifier.pdf}
        \end{center}
        \caption{Neural Network}
    \end{subfigure}
    \caption{Accuracy and F1-Scores of all of our three models in both clear and FHE on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_acc_f1}
\end{figure}

As expected (see \cref{LogisticRegressionExp}) the Logistic Regression model quality did not suffer from compiling it to its \ac{fhe} equivalent.
For all datasets, the accuracy and F1-scores stay exactly the same while also having a no standard deviation (since we fixed the seed for this experiment).
This proofs that with a high quantization bit-width and a very shallow \ac{fhe} inference circuit one can achieve perfect accuracy of the \ac{fhe} model.

Our \ac{cart} model (see \cref{XGBExp}) only suffers from an extremely small accuracy loss due to quantization which is not viewable on the plot.
While this was better than expected, we also have to note that the \ac{pca} that we used in this experiment might have helped a lot here.
In fact it might have even been much too aggressive since in theory any \ac{cart} model must be at least equally as good and most of the time better than Logistic Regression.
What we observed instead were changing improvements over Logistic Regression on some datasets (xor, most spam variants), but a worse performance than Logistic Regression on the others.
As with most experiments we implemented them as close as the example code from the concrete-ml documentation.
In this case however the recommended usage of \ac{pca} probably crippled the model quite a bit to the point were using it over Logistic Regression makes no sense considering its increased runtimes.

For our \ac{fnn} model (see \cref{neural_net_exp}) experiments we did not use a fixed seed since it is not based on scikit-learn anymore and thus concrete-ml does not expose an attribute for doing this.
As a result our accuracies and F1-scores were subject to variation from run to run which can be seen in the error bars.
The \ac{fnn} model has slightly better accuracies across the board, except for the spam and cancer datasets where it is slightly behind Logistic Regression.
The differences are so small however that they are mostly insignificant hinting at the fact that our datasets are not challenging enough to show the differences in model performance well.
Notably the neural network is the only model having acceptable accuracy in our XOR dataset.
As expected the accuracy drops quite much after compiling the model to \ac{fhe}.
How much was highly dependent on the dataset.
While the digits and spam datasets where barely affected, the \ac{fhe} model performed much poorer than the clear model on the cancer and xor datasets which are very challenging for the quantization.

Except for smaller variations within the margin of error, the PC run returned the same accuracy and F1-scores.

\subsection{Runtime}

A full overview of all runtime data is available in \cref{table:inference_cluster_dur} (cluster run) and \cref{table:inference_PC_dur} (PC run).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_Logistic Regression.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_Logistic Regression.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the Logistic Regression model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_logReg}
\end{figure}

The runtime increases due to \ac{fhe} of our Logistic Regression experiment (see \cref{fig:cluster_runtime_logReg}) started out to be relatively modest on datasets with smaller feature sizes with slowdowns around $100\times$.
However even this kind of slowdown means in practice that an extremely trivial operation like a vector product and addition (i.e. inference of a Logistic Regression model) that (for the whole test dataset) usually takes single- to low two-digit milliseconds becomes an operation taking a good fraction of a second, or even more than a second.
This also does not include the pre-processing (e.g. encryption) cost, that is higher than the clear processing by similar factors.
Interestingly the relative slowdown increases drastically with larger feature sizes, suggesting that computing a vector product in \ac{fhe} becomes increasingly inefficient with larger dimensions.
The PC run (see \cref{fig:PC_runtime_logReg}) was faster for the clear execution than on the cluster, while the \ac{fhe} execution was slower, both by a significant margin.
This increases the slowdown factors even more, suggesting that while the clear model benefits from the stronger single-core performance of the PC, \ac{fhe} is much better parallelized and thus can benefit much more of the high core-count of the cluster.
This is even more apparent for the \ac{fhe} pre-processing.
While post-processing benefits from higher single-core performance and experiences a significant performance improvement on the PC, pre-processing slows down so much more that in total our relative pre+post numbers increase by quite a lot compared to the cluster run.
While most cloud machines offer high multi-core performance that \ac{fhe} processing can take advantage from, the pre-processing runs on client devices where parallelization results in much less of a performance boost.
This means that specifically for the pre+post processing results, the PC run is much more representative for real-world conditions than the cluster run.
Not only does this mean that the pre+post processing is much slower than just running the clear model directly on the client, for Logistic Regression it is also even slower than the actual \ac{fhe} processing, raising the question whether such simple models are even worth running in a homomorphic environment.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_XGB Tree-based Classification.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the XGB tree-based model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_xgb}
\end{figure}

The \ac{cart} models shows a bit of a different runtime behavior (see \cref{fig:cluster_runtime_xgb}).
First of all, the runtime does not increase with dataset feature size because the effective feature dimensionality to (almost) the same value by applying \ac{pca}.
However the Digits dataset still stands out with higher relative runtimes than the rest, suggesting that \ac{fhe} runtimes react much worse to larger class counts than the clear execution does.
The Synthetic dataset on the other hand is the one with the smallest relative slowdown which might have to do with it being easily separable with even very shallow decision trees (even for larger feature sizes).
The slowdown due to \ac{fhe} is also much larger for this model, being a full order of magnitude larger than for Logistic Regression (around $1000\times$ slower now).
This might be due to the necessary parallel execution of all possible branches of the tree in the \ac{fhe} model, or just due to the fact that the amount of computations required is higher than in Logistic Regression (even in a single branch).
At the same time the preprocessing duration has only increased slightly compared to Logistic Regression making the runtime proportions between clear, preprocessing and actual processing much more sensible in this experiment.
We still want to point out though that preprocessing was still by multiple factors slower than the clear execution of the model.
The PC run (see \cref{fig:PC_runtime_xgb}) does not differ that much this time, it essentially just has increased runtimes for everything across the board, including the clear execution this time.
Both the clear and \ac{fhe} executions seem to benefit more from higher core counts, changing the runtime ratios not that much in the process.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot_Neural Network Classifier.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-with-ratio_Neural Network Classifier.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of the Neural Network model on various datasets, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_neural_net}
\end{figure}

For our \ac{fnn} model, we make another jump by an order of magnitude regarding relative slowdown, now being in the $\approx 10,000\times$ range of slowdown and even hitting more than $100,000\times$ on the 100 dimensions variation of Spam (see \cref{fig:cluster_runtime_neural_net}).
This is also the reason why we quit our executions here, the slowdown became higher and higher with more dimensions, exploding this experiments execution times.
However the variance is also much larger here, both between the $10$ executions and between the datasets.
For example the relative slowdown is much lower for the smaller and simpler datasets like Iris, Cancer or our Synthetic dataset.
The Digits dataset, the one with the highest amount of classes, stands out here again because of especially large slowdowns, as well as the SMS Spam dataset variations.
Our PC run was slower across the board, however the \ac{fhe} execution suffered more from the lower core-count than the clear execution making the slowdown factors even worse while also featuring less variance/error across the $10$ executions (see \cref{fig:PC_runtime_neural_net}).

\subsection{How Runtime Grows with Feature Size}

Since we executed our Synthetic and Spam datasets with a variety of feature sizes, we can look at how runtimes grow with the dimensionality of the input features (see \cref{fig:cluster_synth_feature-runtime}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_Logistic Regression_synth_.pdf}
        \end{center}
        \caption{Logistic Regression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_XGB Tree-based Classification_synth_.pdf}
        \end{center}
        \caption{XGB Tree-based}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_feature-runtime-plot_Neural Network Classifier_synth_.pdf}
        \end{center}
        \caption{Neural Network}
    \end{subfigure}
    \caption{Runtimes plotted against feature size of the 'Synthetic' dataset of all of our three models in both clear and FHE, plotted on a logarithmic scale, drawn from the cluster run of the experiments}\label{fig:cluster_synth_feature-runtime}
\end{figure}

While clear execution and post-processing are not affected that much by the feature size, \ac{fhe} preprocessing and processing increase linearly and steep with growing feature size.
While of course in absolute values these increases are much higher for the \ac{fnn} model than for the Logistic Regression model, plotted on their respective scales the graphs look quite similar since the relative increases are quite similar.

Our \ac{cart} model is an exception here because of the applied \ac{pca} which normalizes feature size, as the model sees it, to a more or less fixed value.
We suspect that the chaotic changes in execution times are due to the for some dimensionalities the dataset is easier to separate with shallower trees(after applying \ac{pca}) than for others.

These findings also translate to the SMS dataset, as well as the plots for both datasets of the PC run (see \cref{fig:cluster_spam_feature-runtime}, \cref{fig:PC_synth_feature-runtime}, and \cref{fig:PC_spam_feature-runtime} respectively).

We also have to point out though that feature sizes of larger than $250$ are unusual in practice and where integrated into the benchmarks more to push the boundaries of what is possible with this framework, and less to reflect real-world relevant use cases.
We also observe that increasing the feature size over $250$ on the Spam dataset did not increase accuracy anymore, with $100$ and $50$ dimensions only resulting in relatively minor accuracy losses but quite high runtime gains during \ac{fhe} execution with all model types.

\subsection{Changes in the Decision Boundary}

We will now look at the decision boundaries of the trained models to find some visual explanations for the observed accuracy losses of the models.
For this we used \ac{pca} backwards to map 2-dimensional coordinates to features for the dataset (except for XOR where the features already are 2-dimensional).
Unfortunately this did not succeed for the \ac{fnn} model though due to concrete-ml consistently throwing errors during plotting.
We suspect that the underlying cause might here might be the aggressive quantization used for the neural network experiment which restricts most of our \ac{pca}-mapped features to be in the expected input range for proper quantization.
Nevertheless even for Logistic Regression and our \ac{cart} models we see some interesting effects that quantization and \ac{fhe} compilation has on the decision boundary:

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Iris - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Iris - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Iris' dataset}\label{fig:dec_bound_log-reg_iris}
\end{figure}

As you can see in \cref{fig:dec_bound_log-reg_iris}, for Logistic Regression the changes in the decision boundary mostly manifest in the boundary being a bit more granular due to quantization.
There are however exceptions to this rule, as one can see on the breast cancer dataset which has quite large feature dimensions and thus also sees very high changes in boundary granularity (see \cref{fig:dec_bound_log-reg_cancer}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Breast Cancer - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Breast Cancer - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Breast Cancer' dataset}\label{fig:dec_bound_log-reg_cancer}
\end{figure}

Another exception is the digits model, where due to quantization the boundary changes quite drastically at the edges of the feature space (see \cref{fig:dec_bound_log-reg_digits}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Digits - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/Logistic Regression - Digits - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} Logistic Regression model on the 'Digits' dataset}\label{fig:dec_bound_log-reg_digits}
\end{figure}

For the \ac{cart} model, the decision boundaries change much less after compiling the model to its \ac{fhe} equivalent.
This may be attributes to the fact that the boundaries are already quite sharp due to the model design.
Every decision in our decision tree creates a sharp, straight line whether the threshold has been quantized or not.
However quantization can still move these straight lines around a bit, as can be seen on the Digits dataset (see \cref{fig:dec_bound_xgb_digits})

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/XGB Tree-based Classification - Digits - clear.pdf}
        \end{center}
        \caption{clear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/XGB Tree-based Classification - Digits - FHE.pdf}
        \end{center}
        \caption{FHE}
    \end{subfigure}
    \caption{Decision boundaries of the clear and \ac{fhe} XGB tree-based model on the 'Digits' dataset}\label{fig:dec_bound_xgb_digits}
\end{figure}

More decision boundaries of some of our datasets for these two models can be found in \cref{add_dec_bound}.

\section{Encrypted Training with Built-In Model}

These are the results of our second group of experiments, described in \cref{encrypted_training}.

\subsection{Limitations of concrete-ml and Encountered Errors}

\subsection{Accuracy and F1-Score}

\subsection{Runtime}

\subsection{How Runtime Grows with Feature Size}

\section{Encrypted Inference with custom approaches for the \acl{ner} task}

These are the results of our third and final group of experiments, described in \cref{ner_exps}.
Please note that we were only able to run our custom PyTorch model using \ac{ptq}, and our \ac{knn} approach with very unrepresentative and practically useless parameters (see \cref{ner_limitations}).

\subsection{Limitations of concrete-ml and Encountered Errors}\label{ner_limitations}

\subsubsection{Imposed Restrictions to PyTorch Model Design}

We already described our custom model in \cref{custom_pytorch_model}, however originally we built the architecture a bit differently and also achieved slightly better results with it.
We had to simplify the model design however because of compatibility issues with concrete-ml.

Essentially we tried to supply the \ac{fnn}-part of the model with our three types of features separately while using separate embeddings on the token indices and capitalization information and passing the word lengths directly to the model without an embedding.
This original model design can be seen in \cref{code:original_model}.
In theory this should be a cleaner solution since between these three categories the inputs have drastically different meanings and their relation to one another (across these categories) does not matter at all.

While our original approach worked well in clear (even better than our final approach), concrete-ml was not able to compile the model to its \ac{fhe} equivalent.
Concrete-ml does support the PyTorch Embedding layer, but only if it is the very first operation of the forward method.
Since our approach required splitting the features into three separate vectors before passing them to the Embedding layer, it did not compile with an obscure error message.
At the time of writing this restriction is not documented anywhere and we were only able to confirm that this is intended behavior and not a bug of the framework by conferring with a developer of concrete-ml on the FHE.org Discord server.

We worked around this by modifying our features such that there would be no collision between vocabulary indices, capitalization integers, and word lengths (by just using integers larger then the size of the vocabulary for the latter two) and passing all features directly through one shared Embedding layer.

\subsubsection{\ac{qat} not compatible with PyTorch Embeddings}

Initially we wanted to compare \ac{ptq} with \ac{qat} performance on our custom PyTorch model.
However after implementing and training the \ac{qat} version of our model, compiling it into its \ac{fhe} equivalent failed due to the \ac{qat} code not being compatible with the PyTorch Embedding and its integer inputs.
Since most other models take floating point inputs that require different quantization, the PyTorch Embedding requires some special treatment by concrete-ml.
At the time of writing this has not been adapted to the \ac{qat} code yet.
Therefore we were not able to include the \ac{qat} version of our model in the results, however it should be noted that we already tested the \ac{qat} capabilities of concrete-ml by using its Built-In \ac{fnn} model in \cref{encrypted_inference}.

\subsubsection{A Discovered Bug}

A second issue related to PyTorch Embeddings turned up after compiling the model into its \ac{fhe} equivalent while using the client-server setup in the experiment.
Using the custom model in a deployment scenario failed while using the testing method of the model for inference worked.
After consulting the FHE.org Discord server this turned out to be a bug, and was fixed in \href{https://github.com/zama-ai/concrete-ml/pull/1149}{this GitHub Pull Request}.

\subsubsection{Concrete-ml's \ac{knn} Turning Out to be Useless in Practice}

Regarding our second \ac{ner} approach described in \cref{ner_knn-approach} we have to report that we were not able to implement it for parameters that are meaningful for real-world scenarios like our \ac{ner} problem in any way.
Concrete-ml was only able to compile and train our model ones we reduced the training dataset to $13$ samples of $3$ dimensions each (using \ac{pca}).
For reference, \ac{bert} embeddings have 768 dimensions and our \ac{cconll} training set has almost $60,000$ samples.
While reducing these values somewhat, e.g. due to the curse of dimensionality described in \cref{KNN}, might not harm model performance much, these values are of course completely useless for our purposes (or any other real-world purposes for a \ac{knn} model).

While apparently the \ac{knn} model is still quite new and still being worked on in concrete-ml, these limitations are not mentioned in the documentation and we only found out about them after already implementing the experiment.
Because of this and to still find out about the runtime performance of this \ac{fhe} \ac{knn} model we chose to still include this experiment in this work since it might still be representative of feature versions of concrete-ml, or other \ac{fhe} implementations of \ac{knn}.

\subsection{Accuracy and F1-Score}

\Cref{fig:cluster_acc_f1_ner} shows an overview of the accuracy and F1-Scores of two \ac{ner} approaches from the cluster run. The full data is available in \cref{table:ner_cluster_acc-f1} (cluster run) and \cref{table:ner_PC_acc-f1} (PC run).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/cluster_acc_f1-plot-ner.pdf}
    \caption{Accuracy and F1-Scores of our two \ac{ner} approaches in both clear and FHE on the \ac{cconll} dataset, drawn from the cluster run of the experiments}\label{fig:cluster_acc_f1_ner}
\end{figure}

While we expected quite a large decrease of accuracy due to the use of \ac{ptq}, the large drop from an F1-Score of $80.8\%$ to $46.6\%$ still came at a surprise.
As a point reference, the \ac{knn} approach, which cannot really be better than random class selection with these parameters, had a score of $35.9\%$; only about $10\%$ worse then our custom PyTorch model in \ac{fhe}.
This is quite the large gap in model accuracy between clear and \ac{fhe} and would have a considerable impact on the user experience if the \ac{fhe} model were deployed in a production environment.
Since we actually used a larger bit-width than for the \ac{fnn} model experiment we largely contribute this to the usage of \ac{ptq} over \ac{qat} and maybe to \ac{ner} as a use case which might be more sensitive to smaller weight and input changes due to quantization.
After all even smaller changes in the vocabulary index or in the embedding vector could mean a significant semantic distortion for the model.
Our PC run also delivered similar results (see \cref{fig:PC_acc_f1}).

\subsection{Runtime}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-ner.pdf}
        \end{center}
        \caption{Both FHE and clear runtimes in absolute numbers on a logarithmic scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../figures/cluster_runtime-plot-ner-with-ratio.pdf}
        \end{center}
        \caption{The FHE runtimes relative to the clear runtimes on a linear scale}
    \end{subfigure}
    \caption{Runtimes of our two \ac{ner} approaches in both clear and FHE on the \ac{cconll} dataset, drawn from the cluster run of the experiments}\label{fig:cluster_runtime_ner}
\end{figure}
