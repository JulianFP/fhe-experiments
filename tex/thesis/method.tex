\chapter{Method}

In this chapter we will describe the experiment setup used to measure various trade-offs of homomorphic encryption in different machine learning scenarios.
We will describe the technologies and general methodology as well as all the different datasets and model architectures used.

\section{Technological Overview}

The source code of all experiments described in this thesis is open-source and available on GitHub \cite{partanen_julianfpfhe-experiments_2025}.
The repository includes a README file with some basic instructions in how to run the experiments.
This means that the reader may generate every data point or graph in this thesis themselves.
The absolute numbers will differ because of hardware and software environment differences, but the relative numbers as well as general findings should thus be reproducible by anyone.

The experiments were implemented in Python, the de facto standard language in the field of machine learning.
We have added astral's \emph{uv} \cite{astral_uv_nodate} as a project and dependency management tool to make running the script with all necessary dependencies easier and to pinpoint all dependencies to a specific version to increase reproducibility.

We defined common interfaces for all experiments as well as datasets to ease their implementation, reduce code duplication, and to ensure to only change desired attributes between different experiments to increase comparability.

For generating result data we used Python's statistics and csv modules as well as \emph{matplotlib} \cite{noauthor_matplotlib_nodate} to generate all graphics.
The graph generation runs independently from the experiment execution which allows redrawing graphs of existing experiment results to for example change the styling of graphs after the fact.

The python package also uses \emph{click} \cite{pallets_welcome_nodate} to add an easy-to-use \acs{cli} to be able to adjust parameters like execution repetitions, which experiment-dataset combinations should be executed, or which graphs should be generated.

\section{The Examined \acs{fhe} Framework: concrete-ml}

We chose the \emph{concrete-ml} framework to implement a variety of machine learning models in \ac{fhe} \cite{noauthor_concrete_2025}.
It is built on top of the \ac{tfhe} compiler \emph{concrete} which turns Python programs into \ac{tfhe} circuit-equivalents \cite{noauthor_concrete_2025-1}.
Both projects are fully open-source and are being developed by the cryptography start-up Zama.
While concrete-ml and Zama are not the only ones trying to implement \ac{fhe} for machine learning applications, from what we have seen concrete-ml seems to be by far the easiest, most complete, and most high-level solution at the time of writing.
It's main selling points are support for a large variety of models, including custom models, while requiring very little or no cryptography knowledge from the developer implementing these models.
This means that data scientists and in general people with some machine learning experience but no \ac{fhe} or cryptography knowledge should be able to build \ac{fhe} models with concrete-ml without leaving the comfort of Python and Pytorch or scikit-learn.
We will look at some alternatives to concrete-ml later, but during our experiments we exclusively relied on concrete-ml.

\subsection{Feature Overview}

We already explained the \ac{tfhe} scheme to some extend in \cref{TFHE}.
Concrete largely implements this scheme as described in that section, with some extensions to increase it's functionality and flexibility.
It's main innovation was \emph{\acf{pbs}} (which we already mentioned in \cref{TFHE_building_blocks}) which allows to execute functions that can be expressed as a look-up table during the Bootstrapping process.
This \ac{pbs} approach is used a lot by concrete and concrete-ml, in practice it implements most non-linear operations (i.e. everything that cannot be expressed as addition and multiplication with a constant).
When looking at a neural network, many operations are linear like matrix multiplications with the model weights.
However machine learning models have to incorporate non-linear functions to be able to separate non-linear data which means that in order to run these models homomorphically we need to be able to evaluate these non-linear functions in \ac{fhe} as well.
For neural networks, these functions mostly are the activation functions, and in concrete-ml they are mostly implemented using look-up tables during \ac{pbs} \cite{dolev_programmable_2021} \cite{noauthor_concrete_2025-1}.

We will now describe the high-level feature set of concrete-ml and some real-world applications they might proof useful in:

concrete-ml provides 21 Built-In model classes which aim to be direct replacements for their scikit-learn counterparts and also provide the same scikit-learn interfaces.
12 of these models are simple linear models (e.g. logistic regression with different training methods, see \cref{LogisticRegression}), 6 of them are \acp{cart} (see \cref{CART}), 2 of them are \acp{fnn} (see \cref{FNN}), and there also exists one model class for \ac{knn} classification (see \cref{KNN}).
For our experiments we chose one model class from each category to evaluate as much of concrete-ml's feature set as possible without having to test all 21 models.
Some of these model classes provide a \texttt{from\_sklearn\_model} method to initialize the model object from a pre-trained scikit-learn model, others however do not (most notably the \ac{fnn} and \ac{knn} models).
These models need to be trained from the ground up using concrete-ml's model class and \texttt{fit} method \cite{noauthor_concrete_2025-1} \cite{noauthor_concrete-mluse_case_examples_nodate}.

If this is too restricting regarding model architecture and/or re-training requirements concrete-ml also supports custom PyTorch or \ac{onnx} models.
For our experiments we disregarded the \ac{onnx} option and went with custom PyTorch models instead because of PyTorch's overwhelming popularity.
concrete-ml allows to turn a custom self-defined PyTorch model into an \ac{fhe} either before or after training, allowing us to deploy already fully trained neural networks into an \ac{fhe} setting \cite{noauthor_concrete_2025-1} \cite{noauthor_concrete-mluse_case_examples_nodate}.

Finally concrete-ml also promises support for more modern and complex model architectures as well, most notably transformer models like the ones being used in \ac{llm}.
In addition to evaluating these models fully in \ac{fhe}, concrete-ml also allows for a hybrid deployment, where the linear layers are executed homomorphically on the server, and the non-linear activation functions in clear on the client, with the intermediate results being exchanged and de-/encrypted in between.
concrete-ml provides some use-case examples that include running \acs{gpt}-2 completely homomorphically.
While this looks very exciting, we did not include it in our suite of experiments for this thesis because of runtime concerns.
We aimed to make our benchmarks as close to real-world examples as possible which for a \ac{llm} always means generating a considerable amount of tokens for a sizeable amount of input tokens.
Zama's own benchmarks however already show that predicting the next token after 8 input tokens took almost 3 minutes while using a single attention head, and over 14 minutes while utilizing 12 attention heads.
Since every following token would have to take the previously generated tokens into consideration, this runtime would only increase for every following token resulting in unbearable runtimes for larger sets of input/output tokens.
We struggled to come up with real-world use-cases that would strictly require an \ac{llm} while not requiring more than single digit input/output tokens \cite{noauthor_concrete_2025-1} \cite{noauthor_concrete-mluse_case_examples_nodate}.

\subsection{Pre- and Post-Processing}

Our experiment results will include runtimes for \ac{fhe} pre-/post-processing in addition to the processing runtimes.
We introduced this terminology to capture all operations that need to be executed on the client-side before the actual \ac{fhe} circuit can be applied on the inputs, or after the output of the \ac{fhe} circuit can be interpreted.
This of course includes encryption of the inputs during pre-processing, and decryption of the output during post-processing.
However it also includes a couple of other operations as well, most notably quantization and serialization during pre-processing, and dequantization and deserialization during post-processing which we will explain in the following.

We already covered quantization in \cref{quantization}.
Specifically for \ac{ml} applications and in concrete-ml, quantization has to be done with both inputs and model weights since they all are almost always floating point values.
The good aspect of this however is that in \ac{ml} the lost precision does often not result in false computations, but just in minor differences in output probabilities.
However if we already are close to the decision boundary, for classification problems this can mean more false classifications, and also regression models will have more inaccuracies due to quantization.
How much a model accuracy is affected by quantization heavily depends on the type of model in use and how the input data is shaped.
For example linear models tend to suffer a lot less from quantization because the error introduced by it is not accumulated across many operations or model layers.
In concrete-ml, quantization is also a lot less aggressive for linear models since these do not use look-up tables which cannot work with high prevision integers since they would drastically increase the size of these look-up tables (we already mentioned the effect of \ac{pbs} on quantization bit-width in \cref{quantization}) \cite{noauthor_concrete_2025} \cite{noauthor_concrete_2025-1}.

Specifically for neural networks, concrete-ml offers two types of quantization: \ac{ptq}, and \ac{qat}.
We will not explain these quantization algorithms in detail, it is however important to point out that, as the name suggests, \ac{ptq} can be applied after training while \ac{qat} executes during training introducing the requirement of re-training any given model from scratch.
\ac{qat} is on the other hand considered to return more optimal quantization resulting in less accuracy loss.
This is why, as already mentioned, some model types is concrete-ml do not allow to be imported directly from an already trained model but require pre-training (e.g. the built-in NeuralNet classes) \cite{noauthor_concrete_2025}.

It is worth noting that quantizing \ac{ml} models is a common practice even outside the \ac{fhe} space, mostly with the goal to reduce model sizes and thus hardware requirements for running them.
concrete-ml uses the Brevitas library for performing \ac{qat} on neural networks which is not an \ac{fhe}-specific library \cite{noauthor_concrete_2025} \cite{noauthor_xilinxbrevitas_2025}.
However in \ac{fhe} and for non-linear models like neural networks, this quantization must be quite aggressive since concrete also has limits to what bit-widths it can support.
Because of this we will always measure accuracy and F1 scores during our experiments to see how big of an impact the quantization has on any given models accuracy on each dataset.

After quantization and encryption, the client has to also perform serialization.
This is the step of encoding our data into a common format that can be send over the network to the server and includes all the required metadata so that the server can read and interpret the ciphertexts correctly.
These three steps also have to be reversed on the output returned by the server in reverse order (post-processing) \cite{noauthor_concrete_2025} \cite{noauthor_concrete_2025-1}.

\subsection{Setup and Usage}

The most prominent problem that concrete-ml tries to solve is encrypted inference, i.e. we deploy an already trained \ac{ml} model on a relatively powerful server machine, and users can encrypt inputs, send them to the server, and receive encrypted outputs of the model without the server being able to decipher either of them.
For this we would use concrete-ml to compile a model into an \ac{fhe} equivalent model, and export that into two archives: A server.zip and a client.zip.
The first one contains the model and everything the server needs to run it homomorphically.
The second one contains cryptographic parameters and descriptions of all required pre-/post-processing that needs to be done by the client (but reveals no information about the model or it's weights).
An example for how this process looks can be seen in \cref{code:compile_model}.

\begin{lstlisting}[language=Python, caption=Simple example of compiling an already trained logistic regression model into the FHE equivalent, label=code:compile_model]
    # executed by the model developer in order to generate a server.zip and client.zip
    fhe_model = LogisticRegression.from_sklearn_model(model, example_input_data, n_bits=8)
    fhe_model.compile()
    dev = FHEModelDev(path_dir=<path where server.zip and client.zip will be saved>, model=fhe_model)
    dev.save()
\end{lstlisting}

The example\_input\_data is required so that concrete-ml can automatically figure out the best quantization parameters for the model.
It can be an excerpt from the training data, or some randomly generated data within the same boundaries as the training data.
After this the model can be deployed to the server that will be used to run it with encrypted user inputs.
An example of how a deployment of this can look can be seen in \cref{code:client_server_inference}.

\begin{lstlisting}[language=Python, caption=Client/Server example of encrypted inference, label=code:client_server_inference]
    # on the server-side: Init server object from a server.zip
    server = FHEModelServer(path_dir=<path to server.zip>)
    server.load()
    # send client.zip to our client

    # client-side: Init client object from received client.zip and encrypt input
    client = FHEModelClient(path_dir=<path to client.zip>)
    serialized_evaluation_keys = client.get_serialized_evaluation_keys()
    encrypted_input = client.quantize_encrypt_serialize(input)
    # send serialized_evaluation_keys and encrypted_input to the server

    # server-side: Run model inference in FHE on encrypted input
    encrypted_result = server.run(encrypted_input, serialized_evaluation_keys)
    # send encrypted_result back to our client

    # client-side: decrypt result
    result = client.client.deserialize_decrypt_dequantize(encrypted_result)
\end{lstlisting}

Please note that the serialized\_evaluation\_keys can be viewed as the public key of the client and importantly cannot be used by the server to decrypt the inputs.
They are needed for example for the bootstrapping process (for how the bootstrapping key is used see \cref{Bootstrapping}).
While concrete-ml also offers all-in-one functions to run inference on \ac{fhe} models homomorphically, in our experiments we always implemented this client-server setup to simulate how a real-world deployment as closely as possible.

\section{Experiments and Models}

With our experiments, we aim to find out how well concrete-ml can fulfill real-world use cases, especially while looking at the main two culprits of \ac{fhe} execution which are runtime performance and degraded model accuracy and drawing comparisons both between models and datasets as well as to regular clear executions of the same experiment.

To test the feature set promised by concrete-ml as exhaustively as possible, we implemented a variety of tests which generally can be split in three categories.

\subsection{Encrypted Inference with Built-In Models}\label{encrypted_inference}

Our first group of experiments tests the off-the-shelf built-in models provided by concrete-ml themselves on a set of simpler classification datasets across multiple real world problems like image and document classification (see section \cref{datasets}).
We expect these models to perform relatively well since they are purpose-built to be used in \ac{fhe}, architecturally simpler, and since they only need to solve relatively easy classification problems.
For every model/dataset combination in this category we follow the same basic steps:

\begin{enumerate}
    \item Train our model in clear on the train dataset
    \item Compile the trained clear model to the \ac{fhe} equivalent, or train the untrained \ac{fhe}-equivalent model on the same train dataset (depending on model in use)
    \item Run inference using the clear model on all samples in the test dataset while measuring the required time for completing this step
    \item Run the required \ac{fhe} pre-processing (quantization, encryption, serialization) on all the samples in the test dataset while measuring the required time for completing this step
    \item Run inference using the \ac{fhe} model on all encrypted test dataset samples while measuring the required time for completing this step
    \item Run the required \ac{fhe} post-processing (deserialization, decryption, dequantization, some last operation that needs to run in clear like an argmax) on all the encrypted results from the previous steps while measuring the required time for completing this step
    \item Return the four measured timings as well as accuracy and F1 scores for both clear and \ac{fhe} execution
\end{enumerate}

These steps are then repeated 10 times for each model/dataset combination (including the model training), and we compute the average values as well as standard deviations for all of the returned metrics across these 10 executions.

\subsubsection{Logistic Regression}\label{LogisticRegressionExp}

This model represents the set of linear models that concrete-ml has built-in support for.
We trained it using the LogisticRegression model from scikit-learn, and then used the \texttt{from\_sklearn\_model} method of the concrete-ml object to turn the trained model into it's \ac{fhe} equivalent, using a relatively high bit-width of 8.
This is possible because this  model is linear and does not require table look-ups.
In fact it is even implemented in a leveled way, meaning it should not require Bootstrapping at all.
The operations that are executed inside \ac{fhe} are just one matrix multiplication with the model weights and an addition with the model bias (see \cref{LogisticRegression} for a full explanation) \cite{noauthor_concrete_2025}.

Because of this we expect the runtime overheads to be relatively low compared to the other models, and the accuracy loss due to quantization almost non-existent.
However with it being a very simple and only a linear model, we also expect it to perform quite poorly on more complex problems that require non-linearity to solve them.

\subsubsection{XGB Tree-based Classifier}

This XGBClassifier class is our representative for \ac{cart} models, and it is concrete-ml's equivalent class to the XGBClassifier from the XGBoost library that implements the Boosting algorithm (see \cref{CART}).
We train the XGBoost model together with a standard scaler and \ac{pca} for dimensionality reduction in a scikit-learn pipeline as recommended by the concrete-ml documentation.
This is something that probably would not be necessary if executed in clear, but we found that it also did not change the models accuracy much so we chose to follow this recommendation which just improves \ac{fhe} execution times.
We then again compiled the model into the \ac{fhe} equivalent using the \texttt{from\_sklearn\_model} method \cite{noauthor_concrete_2025}.

As mentioned in \cref{no_control-flow}, \ac{tfhe} (and \ac{fhe} schemes in general) do not allow control flow statements that depend on the input, and thus the implementation of a decision tree where every node in the tree introduces a new branch becomes anything else than trivial.
The strategy for implementing \ac{cart} models used by Zama in concrete-ml is outlined in \citetitle*{frery_privacy-preserving_2023} \cite{frery_privacy-preserving_2023}.
The core strategy comes down to replacing conditions with a parallel evaluation of all branches, and to retrieve the correct result by looking up the decision path afterwards.
While this sounds very inefficient, it actually is not that uncommon even outside the \ac{fhe} space.
This is because the way this parallelized execution of all branches is achieved is by computing all branching decisions simultaneously using tensor operations.
This allows for acceleration with GPUs which in turn can significantly boost performance.
There are libraries like Microsoft's Hummingbird that aim to do exactly this in order to be able to accelerate these more classical machine learning models with tensor operations on GPUs \cite{noauthor_microsofthummingbird_2025}.

In addition to this parallel branch execution it is also required to train the \ac{cart} model using already quantized data so that it can learn the decisions for the correct integers.
Since this model also includes a mix of linear and non-linear operations implemented with \ac{pbs}, it also requires carefully chosen (partially through benchmarking) cryptographic parameters to reduce the quantization error to a minimum \cite{frery_privacy-preserving_2023}.
Nevertheless we were still able to perform the quantization with a bit-width of 8 bit, the same as used for Logistic Regression.
This high bit-width was probably only achievable by using the PCA and scaler operations that reduce the input dimensionality and thus the required FHE circuit depth. Both low quantization bit-width and aggressive usage of PCA reduce model accuracy, balancing the two might proof to be an interesting optimization opportunity.

Since we are running our experiments on CPU, we expect this to how a significant impact on performance, much more than with Logistic Regression.
On the other hand we also expect the \ac{cart} model to significantly outperform Logistic Regression since it is able to separate non-linearly, as long as the data is still clustered by class.
Due to the non-linear operations we expect some measurable decreases in accuracy, however still to a relatively low level compared since this model should not require very deep circuits.

\subsubsection{Neural Network Classifier}\label{neural_net_exp}

The NeuralNetClassifier class implements a \ac{fnn} mimicking the scikit-learn class (as described in \cref{FNN}) and interface with the same name while in reality it's implemented with PyTorch.
For hyperparameters we used 3 hidden layers with the same dimensionality as the input features, the \ac{relu} activation functions, and 100 training epochs with a learning rate of $0.01$.
For this builtin-in model concrete-ml requires training of the FHE model instead of compiling a trained clear model \cite{noauthor_concrete_2025}.
For this we used the \texttt{fit\_benchmark} method that trains the FHE model together with the equivalent clear model for direct comparison between the two.

For quantization we left the bit-width on the default which is 3 bits for both model weights inputs/activations.
Concrete-ml also includes some model specific quantization optimizations like \texttt{power\_of\_two\_scaling} which sets the quantization scale to a power two, making some operations more efficient by turning them into a simple bit-shift. Coupled with the \ac{relu} activation function, this speeds up inference.
Quantization is also further optimized with the use of \ac{qat} which is the main reason why this model has to be trained from scratch \cite{noauthor_concrete_2025}.

Nevertheless this bit-width is still much lower than what we used with the previous two models which is why we expect some noteworthy accuracy reductions of the FHE model compared to the clear model here.
The required computations are also significantly higher than for the previous models and include applying matrix products as well as the non-linear \ac{relu} function for each of the 3+1 layers.
Therefore we expect this neural network model to both have the highest inference times in general as well as the highest relative increase of inference time in FHE compared to the clear model.

\subsection{Encrypted Training with Built-In Model}

This second experiment group tests the use case of encrypted training.
Here instead of training a model on clear data and then performing inference on encrypted data we already perform the training on encrypted data.
This might be interesting in fields where training data is very sparse and the only available data is confidential and cannot be shared with the entity performing the model training or machine learning service.
It allows a training-as-a-service deployment, where e.g. people working in medical fields can train models on confidential patient data on external infrastructure without exposing this data to this infrastructure provider.
Of course using non-sensitive training data should always be preferred over this FHE approach, but the nature of some applications preclude the existence of non-sensitive training (e.g. a model to find brain tumor needs to be trained on brain scans which cannot be anonymized).
This will be tested on the same datasets and thus use cases as \cref{encrypted_inference}, however the experiment steps and the measured attributes differ slightly:

\begin{enumerate}
    \item Train our model in clear on the train dataset while measuring the required time for completing this step
    \item Run the required \ac{fhe} pre-processing (quantization, encryption, serialization) on all the samples in the train dataset while measuring the required time for completing this step
    \item Train the untrained \ac{fhe}-equivalent model on all encrypted train dataset samples while measuring the required time for completing this step
    \item Run the required \ac{fhe} post-processing (deserialization, decryption, dequantization) on the trained model weights and bias while measuring the required time for completing this step
    \item Run inference using the model trained in clear on all samples in the test dataset
    \item Run inference using the model trained with \ac{fhe} on all samples of the same test dataset
    \item Return the four measured timings as well as accuracy and F1 scores for both models
\end{enumerate}

\subsubsection{\acs{sgd} Classifier}

Currently the only model that support being trained on encrypted data in the concrete-ml framework is the \acl{sgd} Classifier/Regressor. As described in \cref{LogisticRegression}, this is just a Logistic Regression model with a different training algorithm.

We fitted the FHE circuit with some randomly generated sample data so that it learns the boundaries of the input values for proper quantization.
Thus the model only ever saw the real training data in encrypted form.
We also limited the model to $50$ training iterations, initialized the weights and bias with zeros, and used a batch size of $8$.
Each batch as well as the start weights and bias got encrypted and passed to the model separately which means that FHE does not break the online learning properties of \ac{sgd}.
After training we decrypt the resulting weights and bias, and initialize a new SGDClassifier from scikit-learn with these weights and bias.

As with the Logistic Regression experiment (see \cref{LogisticRegressionExp}) we used a very high bit-width of $8$ for quantization.
We expect some accuracy loss due to the use of \ac{sgd} with limited iterations in training which might result in a model that is only approximately fitted optimally on the training data.
Accuracy losses due to quantization on the other hand should be very low.
Computing and applying gradients is a computationally quite heavy operation compared to the previous inference tasks.
Also this needs to be done on the training data set and not on the test data set which in our experiment suite usually contains four times as many samples (see \cref{datasets}).
As a result we expect the runtimes of this encrypted training experiment to be very high even though the resulting model architecture is very simple.

\subsection{Encrypted Inference with custom approaches for the \acl{ner} task}\label{ner_exps}

Our third and final experiment group aims to push the boundaries of what is possible with \ac{ml} in \ac{fhe}.
While our previous experiments used relatively simple classification problems as use cases while mainly focusing on how \ac{fhe} impacts runtime performance and model accuracy, these experiments will try to scout out if \ac{fhe} is practical for more demanding use cases as well.

For this we chose the \ac{nlp} task \acf{ner} as a representative for a more demanding task requiring more complex model architectures.
As described in \cref{ner}, \ac{ner} can be a quite demanding task requiring the model to learn the semantics of the input as well as many different rules regarding the context of a token which necessitates a certain parameter size of the model.
At the same time \ac{ner} is not nearly as demanding as more currently relevant tasks like large language modeling done by \acp{llm} while still being somewhat similar since both tasks require modeling language to some extend.

The experiment steps are equivalent to the ones in \cref{encrypted_inference} yet with much more flexibility in difference in the details since we implemented two approaches that try to solve the \ac{ner} task very differently.

\subsubsection{Custom PyTorch model with Embedding layer}

At the core of this approach we coined the \ac{ner} task into a classification problem by hiding the semantic complexities behind a semantic word embedding (see \cref{semantic_embed}), and then solving this classification problem using a simple \ac{fnn}.
We solved the problem of context with the sliding window approach:
The model has to classify one token at a time, and receives a fixed number of tokens before and after this token. We settled with a window size of $5$, meaning the model receives the token that it should classify and two tokens before and after respectively.

To be able to feed our text input to a model, we turned each token into an index by first regularizing the tokens and then assigning integers to them in order of appearance in the training data.
The regularization step is necessary to avoid situations where the same token gets assigned two different integers because of slight differences (e.g. different capitalization, additional spacings, ...).
Because of this the regularization includes making all characters lower-case.
We did not need to perform tokenization since the dataset in use was already tokenized. Refer to \cref{cconll} for more information about the dataset we used.

For cases where our model encounters a token it has never seen during training we created the '<UNK>' token. All unknown tokens in a sample will be replaced with this token before the sample gets converted to token indices.
To help the model learn how to handle samples that include his '<UNK>' token we randomly replaced input tokens with our unknown token during training (with probability $0.05$ for each token such that different tokens would get masked in every epoch).

To turn the training data into sliding window samples we also introduced the '<PAD>' token to the model.
If the entity in question is one of the first or last two tokens in a sample we still feed the token that should be classified as the center token and replace the empty positions with '<PAD>'.
For example if our input is "Paris is the capital of France." (our example from \cref{ner}), and the model should classify 'Paris', it gets the sliding window "<PAD> <PAD> Paris is the" as input.
We can already see some deficiencies with this sliding window approach here since the model often does not get enough context (the quite important "capital of France" is cut out).
Nevertheless we still wanted to start of with a relatively small window size of 5 to keep model complexity low for the \ac{fhe} execution, with the option to increase it in the future.

Since we converted all tokens to lower-case, and since we the model only receives token indices and not the actual tokens, we added two auxiliary features to give the model some attributes of each of the tokens in addition to their indices: Capitalization information and token lengths.
The capitalization information can distinguish between a token
\begin{itemize}
    \item which exclusively consists of capital letters
    \item which starts with a capital letter
    \item which contains a capital letter somewhere
    \item having no capital letters
\end{itemize}
, resulting in 4 capitalization classes.
The word length is just the number of characters in the token, capped at $20$ for regularization.

Our PyTorch model architecture looks like this:
The 15-dimensional feature vector (token index, capitalization class, and token length for each of the 5 tokens in our sliding window) then gets passed through an embedding layer that generates a 128-dimensional embedding vector.
This then gets passed through a regular \ac{fnn} with 3 hidden layers with the following dimensions: 128, 128, and 64.
After each of these layers we used the \ac{relu} activation function as well as a dropout layer with a dropout rate of $0.2$ to reduce overfitting during training.

When only looking at the PyTorch features in use, this custom model only differs from the built-in model in two points:
It utilizes the PyTorch Embedding and Dropout layers.
While concrete-ml does not support all PyTorch layers and features, it does support these two.
Good support for the Embedding layer is especially very crucial since it is by far the most essential layer in our model design and is also a very elementary building block for more complex model architectures like transformer models.
The Dropout layer is only active during training and thus has only a chance to cause problems in \ac{fhe} conversion when \ac{qat} is being used.

We implemented the conversion of the model to \ac{fhe} using both \ac{ptq} and \ac{qat} with a quantization bit-width of 5.
We performed \ac{ptq} by first training the PyTorch model in clear and then using the \texttt{compile\_torch\_model} function of concrete-ml on it.
For this we used some randomly generated sample data with the same minimal and maximal bounds as the actual dataset.
For the \ac{qat} variant of this experiment we used the Brevitas \cite{noauthor_xilinxbrevitas_2025} library to replace all PyTorch layers of the model with Brevitas equivalents, as well as an additional \texttt{QuantLinear} layer at the beginning.
However we were not able to get this \ac{qat} variant to run at all for reasons we will explain in our results.

We intentionally pushed the bit-width for this model to it's limit (the fhe compilation would fail for anything higher than 5) since pushing the boundaries of \ac{fhe} was our goal here.
Because of this we expect even higher runtimes then for our \ac{fnn} experiment (see \cref{neural_net_exp}) while suffering from less performance degradation.
We will see if this trade-off will be worth it.
We expect the Embedding layer to cause a measurable but manageable runtime increase since during inference it can be collapsed into look-up tables.

\subsubsection{Transformer Embeddings with \acs{knn} Classifier}

Our second approach for solving \ac{ner} was inspired by how Apple implemented \ac{fhe} for image recognition on iPhones \cite{noauthor_combining_nodate}.
Their application is recognition of prominent landmarks or other points of interest in the pictures taken by the user, so that the user can search their photo library by picture contents.
For this the iPhone has an on-device \ac{ml} model that detects if there is any landmark on any given picture, and if yes computes a visual embedding for that picture.
This embedding then gets quantized using a bit-width of 8 bits, encrypted (Apple's \ac{fhe} approach also requires quantization), and sent to an Apple server.
The server then looks up this vector in a large database of visual embeddings of pictures of landmarks using a \ac{knn} algorithm and returns encrypted matches, all computed homomorphically (see \cref{KNN}).
The iPhone receives these matches and decrypts them for sorting and categorizing the users local image library.
While Apple uses their own implementation of the \ac{bfv} scheme (see \cref{path_to_modern_schemes}) and thus a very different tech-stack than the \ac{tfhe}-based concrete-ml library we use for our experiments, we wanted to test a similar setup \cite{noauthor_combining_nodate}.
The idea behind this was that if this approach works so well that Apple chose to build a service around it and roll it out to their customers, it must be viable and work very well in practice.
It also cannot be too inefficient since otherwise Apple would not be able to offer this feature for free to all their iPhone customers (even if the feature is only opt-in).

So we transferred the basic idea of computing high-quality embeddings in clear client-side and then only perform a \ac{knn} search homomorphically on the server to the realm of \ac{nlp} and specifically \ac{ner} by computing semantic word embeddings instead of visual embeddings (see \cref{semantic_embed}).
For this we used the \ac{bert} model (the 'base-cased' variant) \cite{devlin_bert_2019} to generate word embeddings for the token that we want to classify in our \ac{ner} setting, capturing the whole context of the input sample.
Using a transformer encoder model like \ac{bert} should produce high-quality semantic embeddings and is possible in this setting since this happens client-side and we are thus not restricted by limitations of \ac{fhe} execution here.
Similar to Apple's approach, these embedding vectors then get quantized and encrypted for homomorphic a \ac{knn} search using concrete-ml's built-in \ac{knn} model.

As already described in \cref{KNN}, \ac{knn} suffers from the curse of dimensionality.
Apple approached this by clustering the vectors of landmarks into multiple smaller vector databases while having the iPhone tell the server which cluster to use for the \ac{knn} search \cite{noauthor_combining_nodate}.
This comes at the cost of slightly reduced privacy since the server now learns the broad category of the landmark that the image contains, as well as additional required client-side computations.
We did not implement any such mechanism for our experiment which might result in it performing poorly.
\ac{bert} generates embedding vectors with 768 dimensions \cite{devlin_bert_2019}, and the \ac{knn} would need to be performed on order of magnitude of ${\sim}10,000$ training samples to be effective for the \ac{ner} task.
This might be challenging for a \ac{knn} search on it's own, and even more so in an \ac{fhe} environment.
Because of this we added a dimensionality reduction with \ac{pca} so that we could freely change the embedding vectors dimension.
There is however a limit how far the vector's dimensionality can be reduced while still keeping it's functionality as a semantic embedding intact.

\section{Datasets}\label{datasets}

We used a variation of datasets across different classification problems commonly solved with a machine learning approach.
Some of these datasets are synthetic for maximum control over their characteristics to specifically measure a models behavior under certain conditions, while others are real-world datasets that we used to measure how well the models perform solving a variety of real-world problems, mimicking a production deployment of a homomorphic machine learning setup as much as possible.

All datasets use single-precision floating point numbers to store feature vectors to make them compatible with all concrete-ml models since some of these models don't support double-precision for their training data.
We made this choice to increase comparability both between \ac{fhe} and non-\ac{fhe} executions as well as across different model architectures and since the accuracy trade-off should be almost non-existing.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{Y||Y|Y|Y|Y|Y|Y|Y}
        & \textbf{XOR} & \textbf{Iris} & \textbf{Cancer} & \textbf{Digits} & \textbf{Synth} & \textbf{Spam} & \textbf{\aca{cconll}} \\
        \hline
        \hline
        \textbf{feature dims} & 2 & 4 & 30 & 64 & 50--2500 & 50--2500 & - \\
        \hline
        \textbf{feature spread} & 0.921 & 5.7 & 4030.4 & 16.0 & 9.578--67.912 & 1.0 & - \\
        \hline
        \textbf{\#train samples} & 40 & 120 & 455 & 1437 & 200 & 4459 & 59388 \\
        \hline
        \textbf{\#test samples} & 40 & 30 & 114 & 360 & 50 & 1115 & 100 \\
        \hline
        \textbf{\#classes} & 2 & 3 & 2 & 10 & 2 & 2 & 5 \\
        \hline
        \textbf{class distribution} & 0.5\slash 0.5 & 0.333\slash 0.333\slash 0.333 & 0.373\slash 0.627 & ${\approx}0.1$ each & 0.496\slash 0.504 & 0.866\slash 0.134 & 0.318\slash 0.15\slash 0.204\slash 0.237\slash 0.09 \\
    \end{tabularx}
    \caption{Quick overview of core attributes of all our datasets}\label{table:all_datasets}
\end{table}

You can see a the core attributes of all our datasets in \cref{table:all_datasets}. This includes the dimensionality of the input feature vectors (feature dims), the maximal spread between features of each dimension in the training set (feature spread), the total amount of training and testing samples (\# train samples, \# test samples), the amount of classes that the model has to classify the samples into (\# samples), and finally the distribution of the samples across the classes for the whole dataset (class distribution).
Also as you can see we used a 80/20 train/test split for all datasets except for the 'XOR problem' and the \acs{cconll} datasets.
We will now continue to describe each of these datasets in more detail.

\subsection{XOR problem}

We handcrafted this tiny dataset for code testing with faster iterations as well as quick experiment runs, however it proofed to also be a good metric for how a model handles non-linearity in a dataset, and if a model is able to fit to problem with a very small training set.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../figures/'XOR problem' dataset.pdf}
    \end{center}
    \caption{A plot of the feature space of the 'XOR problem' dataset}
    \label{fig:xor_problem_plot}
\end{figure}

The dataset's feature vectors are 2-dimensional.
The label is 1 if and only if one of the features is closer to 0.75 than to 0.25 while the other is closer to 0.25 than to 0.75.
Conversely if both features are approximately the same then the label is 0.

As the name suggests this mimics the boolean XOR operator, with some differences: The inputs are 0.25 and 0.75 instead of 0 and 1 to keep them between 0 and 1 even with noise applied.
Since some models require more than just 4 samples we also repeated each of them 10 times for both the training and test set resulting in a total of 40 samples each.
To make these repeated samples unique and to add meaningful differences between the training and test set we added some noise to all samples in the form of a random number drawn from an unique distribution between $\pm 0.24$.

The result can be seen in \cref{fig:xor_problem_plot}: A feature space with 4 feature clusters, arranged in a way that makes it impossible for a linear model to separate with an accuracy higher than 75\%.
This makes this dataset great in ensuring that more complex non-linear model actually come with an accuracy improvement.

\subsection{Iris}

The iris plants dataset is a very classical \ac{ml} dataset and is often used as a baseline in benchmarks and experiments.
It's small amount of features and samples make it a great for quick testing and evaluation of \ac{ml} models.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../figures/'Iris' dataset - with PCA applied.pdf}
    \end{center}
    \caption{A plot of the feature space of the 'Iris' dataset, \ac{pca}-reduced}
    \label{fig:iris_plot}
\end{figure}

The task at hand is to classify iris flowers into three different species using the length and width of their sepal and petal as features.
This means that the dataset contains three classes, two of which are linearly separable and the other two are not (this can be seen well in \cref{fig:iris_plot}).
We used scikit-learn's \texttt{load\_iris} function to obtain the dataset \cite{fisher_use_1936} \cite{noauthor_api_nodate}.

\subsection{Breast Cancer}

This dataset is about diagnosing breast cancer by supplying a \ac{ml} model with a set of 30 properties of a patients breasts (e.g. radius, concavity, etc.).
Hence this dataset contains only two output classes, cancer and no cancer.
We used scikit-learn's \texttt{load\_breast\_cancer} function to obtain the dataset \cite{wolberg_multisurface_1990} \cite{noauthor_api_nodate}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../figures/'Breast Cancer' dataset - with PCA applied.pdf}
    \end{center}
    \caption{A plot of the feature space of the 'Breast Cancer' dataset, \ac{pca}-reduced}
    \label{fig:breast-cancer_plot}
\end{figure}

This dataset is especially interesting for our experiments for two reasons:
First, it represents a real world use case that could very well benefit from the use of \ac{fhe} since this kind of medical data of patients might be considered sensitive.
Second, the features spread quite heavily and much more than in our other datasets, with a maximum feature spread of $4030.4$.
This can be challenging for our \ac{fhe} setting because a high range between the minimum and maximum feature means much coarser quantization which could very negatively impact model accuracy in the \ac{fhe} setting.

\subsection{Digits}

The digits dataset is the representative for image recognition tasks in our experiment suite.
It contains $8\times8$ integer (range 0-16) pixel images of handwritten digits, resulting in 10 output classes (one for each digit).
We used scikit-learn's \texttt{load\_digits} function to obtain the dataset \cite{xu_methods_1992} \cite{noauthor_api_nodate}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../figures/'Digits' dataset - with PCA applied.pdf}
    \end{center}
    \caption{A plot of the feature space of the 'Digits' dataset, \ac{pca}-reduced}
    \label{fig:digits_plot}
\end{figure}

Not only is this the only dataset in our experiments that comes from the space of visual computing, but it also is the dataset with the highest amount of output classes.

\subsection{Synthetic}

This binary classification dataset is a generated at random using scikit-learn's \emph{make\_classification} function.
Both classes have only one cluster, making it linearly separable and thus even very simple models should have no problem achieving high accuracy scores on this dataset.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{../figures/'Synthetic, 50 features' dataset - with PCA applied.pdf}
        \caption{'Synthetic, 50 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{../figures/'Synthetic, 500 features' dataset - with PCA applied.pdf}
        \caption{'Synthetic, 500 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{../figures/'Synthetic, 2500 features' dataset - with PCA applied.pdf}
        \caption{'Synthetic, 2500 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \caption{Plots showing the feature space of three of our six variations of the 'Synthetic' dataset, \ac{pca}-reduced.}
    \label{fig:synthetic_plot}
\end{figure}

The amount of features in this dataset is variable which allows us to specifically isolate the impact of feature size on model runtime and performance.
We start with 50-dimensional features, and gradually increase the dimensionality up to 2500, resulting in a total of 6 different variations of this dataset.
For each variation, only 10\% of the features are actually informative of the sample's label, while the other 90\% are just generated at random independently from the true labels.
This mainly tests a models resistance to overfitting.

The result can be seen in \cref{fig:synthetic_plot}.
Please note that in contrast to the 2-dimensional 'XOR problem' dataset we had to map these high dimensional features to 2 dimensions in order to plot them.
For this we used scikit-learn's \acf{pca} implementation.
Please note that by doing this, the visual representation of the dataset isn't accurate, for example in the plots it doesn't appear to be separable even though in reality it is.
The high amount of uninformative dimensions contributes to this effect.

\subsection{SMS Spam}

This is our first real-world dataset and we will use it to represent the use-case of document classification during our experiments.
The SMS Spam dataset was developed by \citeauthor{almeida_contributions_2011} \cite{almeida_contributions_2011} and consists of text messages that are labeled as either ham (label '0') or spam (label '1').

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/'SMS Spam, 50 features' dataset - with PCA applied.pdf}
        \caption{'SMS Spam, 50 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/'SMS Spam, 500 features' dataset - with PCA applied.pdf}
        \caption{'SMS Spam, 500 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/'SMS Spam, 2500 features' dataset - with PCA applied.pdf}
        \caption{'SMS Spam, 2500 features' dataset - with \ac{pca} applied}
    \end{subfigure}
    \caption{Plots showing the feature space of three of our six variations of the 'SMS Spam' dataset, \ac{pca}-reduced.}
    \label{fig:sms_spam_plot}
\end{figure}

To extract feature vectors from the documents we use scikit-learn's \ac{tfidf} Vectorizer while filtering English stop words (see \cref{TFIDF}).
Similarly to the 'Synthetic' dataset we also have different variations of this dataset with different feature dimensions ranging from 50 to 2500.
Limiting the amount of features was achieved by ordering the features by term frequency across the corpus and only considering the top x of the 7463-dimensional \ac{tfidf} vectors.

The result can be seen in \cref{fig:sms_spam_plot}.
The same limitation regarding \ac{pca} as mentioned for the 'Synthetic' dataset applies here as well.

\subsection{\acs{cconll}}\label{cconll}

The \ac{cconll} dataset has to viewed a bit separate from the others since we included it specifically for our third group of experiments that try to solve the \ac{ner} problem homomorphically (see \cref{ner_exps}).
It is a cleaned up variant of the \ac{conll} dataset that removes some labeling mistakes and consistency issues of the original \cite{rucker_cleanconll_2023}.

\ac{cconll} comes with already tokenized sentences (and shorter text excerpts like e.g. sport results) and labels each of the tokens into one of the following classes:
\begin{itemize}
    \item 'O': other, i.e. this token is not part of a named entity. We map this to class '0'.
    \item 'B-LOC' and 'I-LOC': the first/an intermediate token of a location entity. We map both of these to class '1'.
    \item 'B-ORG' and 'I-ORG': the first/an intermediate token of an organization entity. We map both of these to class '2'.
    \item 'B-PER' and 'I-PER': the first/an intermediate token of a person entity. We map both of these to class '3'.
    \item 'B-MISC' and 'I-MISC': the first/an intermediate token of a miscellaneous entity, i.e. a named entity that is neither of the above. We map both of these to class '4'.
\end{itemize}

In other words we simplified the \ac{ner} task here by reducing the amount of classes such that our models do not need to recognize whether a token is the beginning of an entity or not.

Since our \ac{ner} approaches look at one token (and it's context) at a time, we had to decrease the size of the dataset and balance it a bit.
Otherwise we would have had as many samples as there are tokens in \ac{cconll}, and since by far the most tokens do not belong to any named entity we would also have an extremely unbalanced dataset.
Instead we chose to include all tokens that are part of an entity, plus an equal amount of non-entity tokens (if the sample had enough non-entity tokens, otherwise less).
This way we did not cut out any entities from the dataset while not flooding our models with too many non-entity training tokens.

The \ac{cconll} also comes already split into a train, dev, and test dataset.
We used the whole training dataset, ignored the dev set, and only chose $100$ samples from the test dataset at random since \ac{fhe} evaluation on this test set would otherwise have taken much too long.
