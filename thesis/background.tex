\chapter{Background}

The topic of this thesis falls into an overlap between the fields of cryptography and machine learning, and as such we will first introduce some concepts from both fields in this chapter. We will not assume anything more than surface-level knowledge in any of the two fields from the reader.

\section{Cryptography Concepts and FHE}

\subsection{Basic Notation and Phrases}

Throughout this work we will use some of the following notations and phrases that are also commonly encountered in other cryptography-related works. We will introduce these concepts informally aiming for a basic understanding of them while linking to formal definitions.

First we need to introduce the concept of randomness in the context of computational theory. For this we will use the definitions of Anjeev Arora's and Boaz Barak's "Computational Complexity: A Modern Approach" \cite{arora_computational_2009}. Many algorithms in Cryptography rely inherently on randomness and thus cannot be modeled by a standard deterministic Turing Machine (TM) with polynomial programs. Instead we need to model their computation using the Probabilistic Turing Machine (PTM) which is an extension of the TM. In contrast to the TM which only has one transition function $\delta$, the PTM has two transition functions $\delta_0$ and $\delta_1$ and chooses at each step at random which one to apply, with probability half to apply $\delta_0$ and half to apply $\delta_1$.

Similarly how $\mathbf{P}$ is the class of decision problems that are solvable in polynomial time by a TM, $\mathbf{BPP}$ is the class of decision problems that are solvable in polynomial time by a PTM. Note that since the TM is a special case of the PTM (the one where $\delta_0 = \delta_1$), and since it is possible to simulate all branches of a PTM with a TM in time $2^{poly(n)}$ it holds that $\mathbf{P} \subseteq \mathbf{BPP} \subseteq \mathbf{EXP}$ (see chapter 7.1 in \cite{arora_computational_2009}).

An alternative definition of a PTM would be to add a second tape to a TM which includes bits that are results of fair coin tosses, i.e. each symbol on the second tape is chosen mutually independent and uniformly at random from the set $\{ 0, 1 \}$, and then the tape is fed to a TM in addition to the regular input tape. From this viewpoint comes a phrasing of referring to the randomness in cryptographic algorithms like encrypt or key generation functions as 'coins'. These independent random coin-flips can be viewed as an implicit input to any such algorithm, and are in practice most commonly obtained by pseudo-random number generators.

Furthermore, as introduced in the lecture notes of Mihir Bellare and Phillip Rogaway \cite{bellare_introduction_2005} in chapter 1.3, we will use the following notation to mean that $i$ is a value chosen uniformly at random from the set $\mathcal{S}$: $i \xleftarrow{\$} \mathcal{S}$. This notation can also be used for algorithms, e.g. $i \xleftarrow{\$} \text{Encrypt}$ which can be viewed as $i$ being a random value from the image of 'Encrypt', or a random result of all possible branches of the PTM-equivalent of the 'Encrypt' algorithm chosen uniformly at random. The \$ denotes the random coins used as input. Note that some literature may also use $R$ instead of \$.

\subsection{A First Somewhat Homomorphic Encryption Scheme}

We will begin by introducing a simple version of an encryption scheme described by Dijk, Gentry, Halevi and Vaikuntanathan in 'Fully Homomorphic Encryption over the Integers' \cite{van_dijk_fully_2010} as an easy to understand example to explain some fundamentals of FHE on before moving on to more complicated and current schemes.

This first scheme encrypts bits, so our plaintext space is defined as $m \in \{ 0, 1 \}$. Our secret key $p$ is an odd integer that is sufficiently large.

We will now encrypt our $m$ into our ciphertext $c$:

\begin{equation}
    c \leftarrow p q + 2 r + m
    \label{eq:dijk_encryption}
\end{equation}

Here $q$ and $r$ are also some integers $\neq p$, sampled at random from some intervals, with $r$ being much smaller than $p$ and $q$.

The ciphertext $c$ can then be decrypted by doing the following:

\begin{equation}
    m \leftarrow \left( c \mod p \right) \mod 2
\end{equation}

This encryption scheme is correct because by calculating $\mod p$ we get rid of the $p \cdot q$ term in \ref{eq:dijk_encryption} while $\mod 2$ nullifies $2r$, with our $m$ staying untouched since it is either 0 or 1 and thus smaller than 2. Effectively we encode our message in an integer by making it even if it is 0, and odd if it is 1, and then we add $q$ times our secret $p$ to it in order in obfuscate the message. $p$ is odd, but $q$ can either be odd or even making the result unpredictable.

\emph{Remark} This is also the reason why $p$ must be odd, if $p$ where even then $pq$ would always be even as well, making the resulting ciphertext even if $m$ is 0, and odd if $m$ is 1, and thus providing no security at all.

To see why this encryption is secure we first reduce the encryption to the following problem: Given polynomially-many samples (in the security parameter $\lambda$)
\begin{align}
    x_1 &= p q_1 + r_1 \\
    x_2 &= p q_2 + r_2 \\
    ... &\\
    x_n &= p q_n + r_n
\end{align}
for a randomly chosen odd integer $p$, find $p$. So in other words the challenger would generate the parameters and then provide the adversary only with $x_1$ to $x_n$, who then would have to find $p$. Compared to our encryption in \ref{eq:dijk_encryption}, the $x_i$ would be many different ciphertexts, and the $r_i$ would be results of $2r + m$.

While this looks very similar to the well-known Greatest Common Divisor (GCD) problem, the key difference here is the random noise $r_i$ which makes the results only approximates of common divisors of $p$. Because of that this variation is called \emph{Approximate} GCD problem (AGCD). While the GCD problem can be efficiently solved using Euclid's algorithm, AGCD with correctly chosen parameters is hard to solve. While we leave the proof of that to \cite{van_dijk_fully_2010}, intuitively this is the case because when applying Euclid's algorithm the noise values $r_i$ amplify each other rendering the result meaningless.

Finally we will cover the most interesting attribute of this encryption scheme: As long as $r$ stays sufficiently smaller than $p$, multiple ciphertexts obtained from \ref{eq:dijk_encryption} are homomorphic in both addition and multiplication which can easily be shown:

\begin{align}
    c_1 + c_2 &= pq + 2 r_1 + m_1 + pq + 2 r_2 + m_2 \\
              &=2 pq + 2 \left( r_1 + r_2 \right) + m_1 + m_2
    \label{eq:dijk_hom_add}
\end{align}

Decryption would first remove the first term with the $\mod p$ operation, and then remove the second term with the $\mod 2$, only leaving $m_1 + m_2$.

\begin{align}
    c_1 \cdot c_2 &= \left(pq + 2r_1 + m_1 \right) \left( pq + 2r_2 + m_2 \right) \\
                  &= p^2 q^2 + 2 r_2 pq + pqm_2 + 2r_1pq + 4r_1r_2 + 2r_1m_2 + pqm_1 + 2r_1m_1 + m_1m_2\\
                  &= p\left(pq^2 + 2r_2q + qm_2 + 2r_1q + qm_1\right) + 2\left( 2r_1 r_2 + r_1m_2 + r_1 m_1 \right) + m_1 m_2
    \label{eq:dijk_hom_mult}
\end{align}

Once again decryption would remove the first term with the $\mod p$ operation, and then the second term with the $\mod 2$ operation, only leaving $m_1 m_2$.

Please note that because of the $\mod 2$ operation, in the case of $m_1 = m_2 = 1$, $m_1 + m_2$ becomes $0$ which is fine since our message space is binary anyway. So when only looking at the plaintext effectively addition is the XOR-operator and multiplication is the AND-operator.

We can also see some deficits of this simple homomorphic encryption scheme: The noise $r$ grows linearly during addition (see the $r_1 + r_2$ term in \ref{eq:dijk_hom_add}), and quadratically during multiplication ($r_1r_2$ in \ref{eq:dijk_hom_mult}). Since these homomorphic evaluations only stay correct while the noise is sufficiently smaller than $p$ as already stated, we can only homomorphically evaluate a limited amount of additions and multiplications before decryption would output an incorrect result. This is why Gentry called these kind of schemes \emph{somewhat} homomorphic encryption scheme in \cite{gentry_fully_2009}, while also providing us with an general algorithm to turn somewhat homomorphic schemes into fully homomorphic ones which we will look at next.

Another deficiency of this scheme is that the ciphertext grows in size during homomorphic evaluation, e.g. after multiplication the 'key-part' of the ciphertext grew quadratically (the $q^2 p^2$ term in \ref{eq:dijk_hom_mult}). In other words this scheme doesn't compactly evaluate circuits, as defined by Gantry in definition 2.1.2 and 2.1.3 \cite{gentry_fully_2009}. Compact Homomorphic Encryption requires the decryption complexity to not by dependent on the circuit depth which in turn requires the ciphertexts and keys (which are the inputs of the decryption algorithm) to not grow with the depth of the circuit.

While this simple scheme is not representative of modern FHE schemes, it helps to understand the goals as well as struggles of FHE in general. Most encryption schemes introduce some sort of noise parameter that is strictly required for the security of the scheme, but also introduces limitations regarding accuracy and/or depth of homomorphic evaluation. Balancing security with these drawbacks as well as handling noise in ciphertexts remain current challenges even with state-of-the-art FHE schemes.

\section{Machine Learning Concepts and Model Types}
